{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üõçÔ∏è E-Commerce Fashion Image Generation\n",
        "\n",
        "This notebook walks you through the complete pipeline for training and generating fashion images.\n",
        "\n",
        "**Two Models:**\n",
        "1. **Projected GAN** - Fast unconditional generation (~2-4 hours)\n",
        "2. **Stable Diffusion + LoRA** - Text-conditioned generation (~4-8 hours)\n",
        "\n",
        "**Hardware:** Optimized for RTX 4060 (8GB VRAM)\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Run Order\n",
        "\n",
        "1. **Environment Setup** - Install dependencies, verify GPU\n",
        "2. **Data Preparation** - Download/prepare DeepFashion dataset\n",
        "3. **Projected GAN Training** - Train unconditional GAN\n",
        "4. **Projected GAN Generation** - Generate random fashion images\n",
        "5. **Stable Diffusion LoRA Training** - Fine-tune with LoRA\n",
        "6. **Stable Diffusion LoRA Generation** - Generate from text prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Environment Setup\n",
        "\n",
        "Install dependencies and verify GPU availability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install main dependencies (run once)\n",
        "# Uncomment and run if you haven't installed yet\n",
        "\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "# Step 2: Install xformers (AFTER the above completes)\n",
        "# This is optional but recommended for 8GB VRAM\n",
        "\n",
        "# !pip install xformers --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify GPU and CUDA\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è CUDA not available! Training will be very slow on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Common imports and setup\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "\n",
        "# Set project root\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "\n",
        "# Add to path\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Device\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Data Preparation\n",
        "\n",
        "Prepare the DeepFashion dataset for training.\n",
        "\n",
        "**Options:**\n",
        "1. **Kaggle** - Download the full dataset (requires API key)\n",
        "2. **Local** - Use your own fashion images\n",
        "3. **Sample** - Create folder structure (add images manually)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data directories\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "RAW_DIR = DATA_DIR / \"raw\"\n",
        "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
        "\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Raw data directory: {RAW_DIR}\")\n",
        "print(f\"Processed data directory: {PROCESSED_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Download from Kaggle (uncomment to use)\n",
        "# Requires: pip install kaggle && kaggle.json in ~/.kaggle/\n",
        "# !python data/download_deepfashion.py --source kaggle\n",
        "\n",
        "# Option 2: Use local images (uncomment and modify path)\n",
        "# !python data/download_deepfashion.py --source local --local-path \"C:/path/to/your/images\"\n",
        "\n",
        "# Option 3: Sample (creates folder structure, you add images manually)\n",
        "# !python data/download_deepfashion.py --source sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check available images\n",
        "image_extensions = {'.jpg', '.jpeg', '.png', '.webp'}\n",
        "\n",
        "def count_images(directory):\n",
        "    count = 0\n",
        "    for path in Path(directory).rglob('*'):\n",
        "        if path.suffix.lower() in image_extensions:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "if RAW_DIR.exists():\n",
        "    num_images = count_images(RAW_DIR)\n",
        "    print(f\"Found {num_images} images in {RAW_DIR}\")\n",
        "    \n",
        "    if num_images == 0:\n",
        "        print(\"\\n‚ö†Ô∏è No images found! Please add images to data/raw/ before proceeding.\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Directory not found: {RAW_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset for both models (resize, crop, generate captions)\n",
        "# Creates: 256x256 for GAN, 512x512 for LoRA\n",
        "!python data/prepare_dataset.py --max-gan-images 5000 --max-lora-images 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify prepared datasets\n",
        "GAN_DATA = PROCESSED_DIR / \"projected_gan\"\n",
        "LORA_DATA = PROCESSED_DIR / \"lora\" / \"images\"\n",
        "\n",
        "if GAN_DATA.exists():\n",
        "    print(f\"‚úì Projected GAN dataset: {count_images(GAN_DATA)} images (256x256)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Projected GAN dataset not found\")\n",
        "\n",
        "if LORA_DATA.exists():\n",
        "    print(f\"‚úì LoRA dataset: {count_images(LORA_DATA)} images (512x512)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è LoRA dataset not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Projected GAN Training\n",
        "\n",
        "Train Projected GAN for **unconditional** fashion image generation.\n",
        "\n",
        "‚è±Ô∏è **Expected time:** ~2-4 hours on RTX 4060"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GAN configuration\n",
        "with open('config/projected_gan_config.yaml', 'r') as f:\n",
        "    gan_config = yaml.safe_load(f)\n",
        "\n",
        "print(\"Projected GAN Configuration:\")\n",
        "print(f\"  Image size: {gan_config['model']['img_size']}x{gan_config['model']['img_size']}\")\n",
        "print(f\"  Batch size: {gan_config['training']['batch_size']}\")\n",
        "print(f\"  Total images: {gan_config['training']['total_kimg']}k\")\n",
        "print(f\"  Mixed precision: {gan_config['training']['mixed_precision']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Projected GAN components\n",
        "from projected_gan.model import Generator, ProjectedDiscriminator\n",
        "from projected_gan.train import Trainer, FashionDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create dataset\n",
        "gan_dataset = FashionDataset(\n",
        "    root=str(GAN_DATA),\n",
        "    img_size=gan_config['model']['img_size'],\n",
        "    augment=True,\n",
        ")\n",
        "\n",
        "gan_dataloader = DataLoader(\n",
        "    gan_dataset,\n",
        "    batch_size=gan_config['training']['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "print(f\"‚úì Dataset size: {len(gan_dataset)} images\")\n",
        "print(f\"‚úì Batches per epoch: {len(gan_dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trainer\n",
        "gan_trainer = Trainer(gan_config, device=DEVICE)\n",
        "\n",
        "# Print model info\n",
        "g_params = sum(p.numel() for p in gan_trainer.G.parameters() if p.requires_grad)\n",
        "d_params = sum(p.numel() for p in gan_trainer.D.parameters() if p.requires_grad)\n",
        "print(f\"Generator parameters: {g_params:,}\")\n",
        "print(f\"Discriminator trainable parameters: {d_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Projected GAN\n",
        "# ‚ö†Ô∏è This will take 2-4 hours!\n",
        "# Tip: Reduce total_kimg in config for faster testing\n",
        "\n",
        "gan_trainer.train(gan_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Projected GAN Generation\n",
        "\n",
        "Generate random fashion images using the trained GAN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained generator\n",
        "from projected_gan.generate import load_generator, generate_images\n",
        "\n",
        "GAN_CHECKPOINT = PROJECT_ROOT / 'outputs' / 'projected_gan' / 'checkpoint_final.pt'\n",
        "\n",
        "if GAN_CHECKPOINT.exists():\n",
        "    G, _ = load_generator(str(GAN_CHECKPOINT), device=DEVICE)\n",
        "    print(f\"‚úì Loaded generator from {GAN_CHECKPOINT}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Checkpoint not found. Using trainer's generator.\")\n",
        "    G = gan_trainer.G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate random fashion images\n",
        "NUM_SAMPLES = 16\n",
        "TRUNCATION = 0.7  # Lower = higher quality, less diversity\n",
        "\n",
        "generated = generate_images(G, num_samples=NUM_SAMPLES, truncation=TRUNCATION, device=DEVICE)\n",
        "\n",
        "# Display results\n",
        "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "for i, (ax, img) in enumerate(zip(axes.flatten(), generated)):\n",
        "    ax.imshow(img.permute(1, 2, 0).numpy())\n",
        "    ax.axis('off')\n",
        "plt.suptitle(f'Generated Fashion Images (truncation={TRUNCATION})', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Stable Diffusion LoRA Training\n",
        "\n",
        "Fine-tune Stable Diffusion v1.5 with LoRA for **text-conditioned** generation.\n",
        "\n",
        "‚è±Ô∏è **Expected time:** ~4-8 hours on RTX 4060"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear GPU memory before loading Stable Diffusion\n",
        "if 'G' in dir(): del G\n",
        "if 'gan_trainer' in dir(): del gan_trainer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load LoRA configuration\n",
        "with open('config/lora_config.yaml', 'r') as f:\n",
        "    lora_config = yaml.safe_load(f)\n",
        "\n",
        "print(\"LoRA Configuration:\")\n",
        "print(f\"  Base model: {lora_config['model']['name']}\")\n",
        "print(f\"  LoRA rank: {lora_config['lora']['rank']}\")\n",
        "print(f\"  Batch size: {lora_config['training']['batch_size']}\")\n",
        "print(f\"  Gradient accumulation: {lora_config['training']['gradient_accumulation_steps']}\")\n",
        "print(f\"  Effective batch: {lora_config['training']['batch_size'] * lora_config['training']['gradient_accumulation_steps']}\")\n",
        "print(f\"  Epochs: {lora_config['training']['num_train_epochs']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import LoRA components and create trainer\n",
        "# ‚ö†Ô∏è This downloads ~4GB model on first run\n",
        "from stable_diffusion_lora.train_lora import LoRATrainer, FashionLoRADataset\n",
        "\n",
        "lora_trainer = LoRATrainer(lora_config, device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LoRA dataset\n",
        "lora_data_dir = PROCESSED_DIR / 'lora'\n",
        "\n",
        "lora_dataset = FashionLoRADataset(\n",
        "    data_dir=str(lora_data_dir),\n",
        "    tokenizer=lora_trainer.tokenizer,\n",
        "    resolution=lora_config['data']['resolution'],\n",
        ")\n",
        "\n",
        "lora_dataloader = DataLoader(\n",
        "    lora_dataset,\n",
        "    batch_size=lora_config['training']['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "print(f\"‚úì Dataset size: {len(lora_dataset)} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train LoRA\n",
        "# ‚ö†Ô∏è This will take 4-8 hours!\n",
        "# Tip: Reduce num_train_epochs in config for faster testing\n",
        "\n",
        "lora_trainer.train(lora_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Stable Diffusion LoRA Generation\n",
        "\n",
        "Generate fashion images from text prompts using the fine-tuned LoRA model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear memory and load generation pipeline\n",
        "if 'lora_trainer' in dir(): del lora_trainer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "from stable_diffusion_lora.generate import load_pipeline, generate_images as generate_sd_images\n",
        "\n",
        "LORA_CHECKPOINT = PROJECT_ROOT / 'outputs' / 'lora' / 'checkpoint-final'\n",
        "\n",
        "if LORA_CHECKPOINT.exists():\n",
        "    pipeline = load_pipeline(str(LORA_CHECKPOINT), device=DEVICE)\n",
        "    print(f\"‚úì Loaded LoRA from {LORA_CHECKPOINT}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Checkpoint not found: {LORA_CHECKPOINT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate from text prompts\n",
        "PROMPTS = [\n",
        "    \"a high quality fashion photo of an elegant black dress\",\n",
        "    \"a fashion photo of a casual white t-shirt, studio lighting\",\n",
        "    \"a professional product photo of blue jeans\",\n",
        "    \"a fashion photo of a red evening gown, luxury\",\n",
        "]\n",
        "\n",
        "NEGATIVE = \"low quality, blurry, distorted, ugly\"\n",
        "\n",
        "if 'pipeline' in dir():\n",
        "    images = generate_sd_images(\n",
        "        pipeline, prompts=PROMPTS, num_inference_steps=30,\n",
        "        guidance_scale=7.5, negative_prompt=NEGATIVE, seed=42,\n",
        "    )\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "    for ax, img, prompt in zip(axes, images, PROMPTS):\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "        ax.set_title(prompt[:35] + '...', fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try your own prompt!\n",
        "MY_PROMPT = \"a fashion photograph of a summer floral dress, bright colors\"\n",
        "\n",
        "if 'pipeline' in dir():\n",
        "    img = pipeline(MY_PROMPT, negative_prompt=NEGATIVE, num_inference_steps=30).images[0]\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(MY_PROMPT)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üéâ Complete!\n",
        "\n",
        "You've trained and used both models for fashion image generation.\n",
        "\n",
        "| Model | Output | Best For |\n",
        "|-------|--------|----------|\n",
        "| **Projected GAN** | Random 256x256 images | Quick prototyping, diverse outputs |\n",
        "| **SD + LoRA** | Text-conditioned 512x512 images | Specific product descriptions |\n",
        "\n",
        "### Checkpoints Saved:\n",
        "- `outputs/projected_gan/checkpoint_final.pt`\n",
        "- `outputs/lora/checkpoint-final/`"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
