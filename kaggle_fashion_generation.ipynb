{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé® Fashion Image Generation with GANs & Diffusion Models\n",
        "\n",
        "This notebook trains two state-of-the-art generative models on the DeepFashion dataset:\n",
        "\n",
        "1. **Projected GAN** - Fast unconditional image generation\n",
        "2. **Stable Diffusion + LoRA** - Text-conditioned image generation\n",
        "\n",
        "## GPU Auto-Detection\n",
        "The notebook automatically detects your Kaggle GPU and optimizes settings:\n",
        "- **T4 (16GB)**: Batch size 16-32\n",
        "- **P100 (16GB)**: Batch size 16-32  \n",
        "- **A100 (40GB)**: Batch size 64+\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q timm clean-fid ninja lpips scipy click\n",
        "!pip install -q diffusers transformers accelerate peft safetensors\n",
        "!pip install -q xformers\n",
        "!pip install -q einops rich\n",
        "\n",
        "print(\"‚úÖ Packages installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# GPU Detection & Configuration\n",
        "# ============================================\n",
        "\n",
        "def detect_gpu_and_configure():\n",
        "    \"\"\"Detect GPU and return optimized configuration.\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"GPU not available! Enable GPU in Kaggle settings.\")\n",
        "    \n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    \n",
        "    print(f\"üñ•Ô∏è GPU: {gpu_name}\")\n",
        "    print(f\"üíæ VRAM: {vram_gb:.1f} GB\")\n",
        "    \n",
        "    config = {'device': 'cuda', 'gpu_name': gpu_name, 'vram_gb': vram_gb}\n",
        "    \n",
        "    if 'A100' in gpu_name:\n",
        "        print(\"üöÄ A100 detected - Maximum performance mode!\")\n",
        "        config.update({'gan_batch_size': 64, 'lora_batch_size': 8, 'lora_grad_accum': 2, 'num_workers': 4, 'gan_img_size': 512})\n",
        "    elif 'V100' in gpu_name:\n",
        "        print(\"üöÄ V100 detected - High performance mode!\")\n",
        "        config.update({'gan_batch_size': 32, 'lora_batch_size': 4, 'lora_grad_accum': 2, 'num_workers': 4, 'gan_img_size': 256})\n",
        "    elif 'T4' in gpu_name or 'P100' in gpu_name:\n",
        "        print(\"‚ö° T4/P100 detected - Balanced mode\")\n",
        "        config.update({'gan_batch_size': 16, 'lora_batch_size': 2, 'lora_grad_accum': 4, 'num_workers': 2, 'gan_img_size': 256})\n",
        "    else:\n",
        "        print(f\"üîß Unknown GPU - Using conservative settings\")\n",
        "        config.update({'gan_batch_size': 8, 'lora_batch_size': 1, 'lora_grad_accum': 8, 'num_workers': 2, 'gan_img_size': 256})\n",
        "    \n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    return config\n",
        "\n",
        "CONFIG = detect_gpu_and_configure()\n",
        "print(f\"\\nüìä Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"   {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ 2. Dataset Preparation\n",
        "\n",
        "This section loads the DeepFashion dataset from Kaggle.\n",
        "\n",
        "**Required**: Add the DeepFashion dataset to your notebook:\n",
        "1. Go to \"Add Data\" ‚Üí Search \"deepfashion\"\n",
        "2. Select a DeepFashion dataset variant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find DeepFashion Dataset\n",
        "def find_fashion_images(search_dirs=['/kaggle/input']):\n",
        "    \"\"\"Find fashion images in Kaggle input directories.\"\"\"\n",
        "    extensions = {'.jpg', '.jpeg', '.png', '.webp'}\n",
        "    all_images = []\n",
        "    for search_dir in search_dirs:\n",
        "        search_path = Path(search_dir)\n",
        "        if not search_path.exists():\n",
        "            continue\n",
        "        for path in search_path.rglob('*'):\n",
        "            if path.suffix.lower() in extensions:\n",
        "                all_images.append(path)\n",
        "    print(f\"Found {len(all_images)} images in {search_dirs}\")\n",
        "    return all_images\n",
        "\n",
        "ALL_IMAGES = find_fashion_images()\n",
        "\n",
        "if len(ALL_IMAGES) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è No images found! Please add a fashion dataset:\")\n",
        "    print(\"1. Click 'Add Data' in the right panel\")\n",
        "    print(\"2. Search for 'deepfashion' or 'fashion'\")\n",
        "    print(\"3. Add the dataset and re-run this cell\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Found {len(ALL_IMAGES)} images!\")\n",
        "    print(f\"Sample paths: {ALL_IMAGES[:3]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directories and prepare datasets\n",
        "OUTPUT_DIR = Path('/kaggle/working/outputs')\n",
        "GAN_DATA_DIR = Path('/kaggle/working/gan_data')\n",
        "LORA_DATA_DIR = Path('/kaggle/working/lora_data')\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "GAN_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "(LORA_DATA_DIR / 'images').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ============================================\n",
        "# ADJUST THESE VALUES TO USE MORE IMAGES\n",
        "# ============================================\n",
        "MAX_GAN_IMAGES = 50000   # Set to None to use ALL images (can be slow to prepare)\n",
        "MAX_LORA_IMAGES = 5000   # LoRA needs fewer images\n",
        "\n",
        "def prepare_dataset(images, output_dir, img_size=256, max_images=10000, with_captions=False):\n",
        "    \"\"\"Prepare images for training.\"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    if with_captions:\n",
        "        images_dir = output_dir / 'images'\n",
        "        images_dir.mkdir(parents=True, exist_ok=True)\n",
        "    else:\n",
        "        images_dir = output_dir\n",
        "        images_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    images = images[:max_images]\n",
        "    print(f\"Preparing {len(images)} images ({img_size}x{img_size})...\")\n",
        "    \n",
        "    metadata = []\n",
        "    captions = [\n",
        "        \"a high quality fashion photograph of clothing, professional product photo\",\n",
        "        \"a fashion product image, studio lighting, white background\",\n",
        "        \"professional fashion photography, elegant clothing, detailed fabric texture\",\n",
        "    ]\n",
        "    \n",
        "    for i, img_path in enumerate(tqdm(images, desc=\"Processing\")):\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            ratio = img_size / min(img.size)\n",
        "            new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))\n",
        "            img = img.resize(new_size, Image.LANCZOS)\n",
        "            left = (img.size[0] - img_size) // 2\n",
        "            top = (img.size[1] - img_size) // 2\n",
        "            img = img.crop((left, top, left + img_size, top + img_size))\n",
        "            \n",
        "            filename = f'img_{i:06d}.jpg'\n",
        "            img.save(images_dir / filename, quality=95)\n",
        "            \n",
        "            if with_captions:\n",
        "                metadata.append({'file_name': filename, 'text': random.choice(captions)})\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    if with_captions:\n",
        "        with open(output_dir / 'metadata.jsonl', 'w') as f:\n",
        "            for item in metadata:\n",
        "                f.write(json.dumps(item) + '\\n')\n",
        "    \n",
        "    print(f\"‚úÖ Dataset ready: {len(list(images_dir.glob('*.jpg')))} images\")\n",
        "\n",
        "# Prepare datasets\n",
        "if len(list(GAN_DATA_DIR.glob('*.jpg'))) < 100:\n",
        "    prepare_dataset(ALL_IMAGES, GAN_DATA_DIR, img_size=CONFIG['gan_img_size'], max_images=MAX_GAN_IMAGES)\n",
        "else:\n",
        "    print(f\"‚úÖ GAN dataset exists: {len(list(GAN_DATA_DIR.glob('*.jpg')))} images\")\n",
        "\n",
        "if not (LORA_DATA_DIR / 'metadata.jsonl').exists():\n",
        "    prepare_dataset(ALL_IMAGES, LORA_DATA_DIR, img_size=512, max_images=MAX_LORA_IMAGES, with_captions=True)\n",
        "else:\n",
        "    print(f\"‚úÖ LoRA dataset exists\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üéØ Part 1: Projected GAN Training\n",
        "\n",
        "Fast unconditional fashion image generation using feature projection from a frozen EfficientNet backbone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Projected GAN - Model Architecture\n",
        "# ============================================\n",
        "\n",
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, z_dim=256, w_dim=256, num_layers=4):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            in_dim = z_dim if i == 0 else w_dim\n",
        "            layers.extend([nn.Linear(in_dim, w_dim), nn.LeakyReLU(0.2, inplace=True)])\n",
        "        self.mapping = nn.Sequential(*layers)\n",
        "    def forward(self, z): return self.mapping(z)\n",
        "\n",
        "class AdaIN(nn.Module):\n",
        "    def __init__(self, w_dim, num_features):\n",
        "        super().__init__()\n",
        "        self.norm = nn.InstanceNorm2d(num_features, affine=False)\n",
        "        self.style = nn.Linear(w_dim, num_features * 2)\n",
        "        self.style.bias.data[:num_features] = 1.0\n",
        "        self.style.bias.data[num_features:] = 0.0\n",
        "    def forward(self, x, w):\n",
        "        style = self.style(w)\n",
        "        gamma, beta = style.chunk(2, dim=1)\n",
        "        return gamma.unsqueeze(-1).unsqueeze(-1) * self.norm(x) + beta.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "class SynthesisBlock(nn.Module):\n",
        "    \"\"\"Synthesis block with SPECTRAL NORMALIZATION for stability.\"\"\"\n",
        "    def __init__(self, in_ch, out_ch, w_dim, upsample=True):\n",
        "        super().__init__()\n",
        "        self.upsample = upsample\n",
        "        # Spectral normalization prevents generator explosion!\n",
        "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
        "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_ch, out_ch, 3, padding=1))\n",
        "        self.adain1, self.adain2 = AdaIN(w_dim, out_ch), AdaIN(w_dim, out_ch)\n",
        "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.noise_scale1 = nn.Parameter(torch.zeros(1))  # Learnable noise scale\n",
        "        self.noise_scale2 = nn.Parameter(torch.zeros(1))\n",
        "    def forward(self, x, w):\n",
        "        if self.upsample: \n",
        "            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        h = self.conv1(x)\n",
        "        h = h + self.noise_scale1 * torch.randn_like(h)  # Safer noise injection\n",
        "        x = self.act(self.adain1(h, w))\n",
        "        h = self.conv2(x)\n",
        "        h = h + self.noise_scale2 * torch.randn_like(h)\n",
        "        return self.act(self.adain2(h, w))\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"Generator with spectral normalization for bulletproof training.\"\"\"\n",
        "    def __init__(self, z_dim=256, w_dim=256, img_size=256, base_ch=32):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.mapping = MappingNetwork(z_dim, w_dim)\n",
        "        self.const = nn.Parameter(torch.randn(1, base_ch * 16, 4, 4) * 0.01)  # Smaller init\n",
        "        self.blocks = nn.ModuleList()\n",
        "        in_ch = base_ch * 16\n",
        "        for _ in range(int(np.log2(img_size)) - 2):\n",
        "            out_ch = max(base_ch, in_ch // 2)\n",
        "            self.blocks.append(SynthesisBlock(in_ch, out_ch, w_dim))\n",
        "            in_ch = out_ch\n",
        "        # Spectral norm on final layer too\n",
        "        self.to_rgb = nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(in_ch, 3, 1)), \n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        w = self.mapping(z)\n",
        "        x = self.const.repeat(z.shape[0], 1, 1, 1)\n",
        "        for block in self.blocks: \n",
        "            x = block(x, w)\n",
        "        return self.to_rgb(x)\n",
        "\n",
        "class ProjectedDiscriminator(nn.Module):\n",
        "    def __init__(self, backbone='tf_efficientnet_lite0', proj_ch=128):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(backbone, pretrained=True, features_only=True, out_indices=[1, 2, 3])\n",
        "        for p in self.backbone.parameters(): p.requires_grad = False\n",
        "        self.backbone.eval()\n",
        "        with torch.no_grad():\n",
        "            dims = [f.shape[1] for f in self.backbone(torch.zeros(1, 3, 256, 256))]\n",
        "        self.projectors = nn.ModuleList([nn.Sequential(nn.Conv2d(d, proj_ch, 1), nn.LeakyReLU(0.2)) for d in dims])\n",
        "        self.heads = nn.ModuleList([nn.Sequential(nn.Conv2d(proj_ch, proj_ch, 3, padding=1), nn.LeakyReLU(0.2), nn.Conv2d(proj_ch, 1, 1)) for _ in dims])\n",
        "    def forward(self, x):\n",
        "        x = ((x + 1) / 2 - torch.tensor([0.485, 0.456, 0.406], device=x.device).view(1, 3, 1, 1)) / torch.tensor([0.229, 0.224, 0.225], device=x.device).view(1, 3, 1, 1)\n",
        "        features = self.backbone(x)\n",
        "        return [head(proj(feat)) for feat, proj, head in zip(features, self.projectors, self.heads)]\n",
        "\n",
        "def hinge_loss_dis(real, fake):\n",
        "    return sum(torch.mean(F.relu(1 - r)) + torch.mean(F.relu(1 + f)) for r, f in zip(real, fake)) / len(real)\n",
        "\n",
        "def hinge_loss_gen(fake):\n",
        "    return -sum(torch.mean(f) for f in fake) / len(fake)\n",
        "\n",
        "print(\"‚úÖ GAN models defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# GAN Dataset & Training Function\n",
        "# ============================================\n",
        "\n",
        "class FashionDataset(Dataset):\n",
        "    \"\"\"Dataset with STRONG augmentation for small datasets.\"\"\"\n",
        "    def __init__(self, root, img_size=256):\n",
        "        self.images = list(Path(root).glob('*.jpg')) + list(Path(root).glob('*.png'))\n",
        "        # Strong augmentation - critical for small datasets!\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(int(img_size * 1.15)),   # Slightly larger\n",
        "            transforms.RandomCrop(img_size),           # Random crop (not center)\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "            transforms.RandomAffine(degrees=8, translate=(0.08, 0.08), scale=(0.95, 1.05)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5]*3, [0.5]*3),\n",
        "        ])\n",
        "        print(f\"üìä Dataset: {len(self.images)} images with strong augmentation\")\n",
        "    \n",
        "    def __len__(self): return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        try: \n",
        "            return self.transform(Image.open(self.images[idx]).convert('RGB'))\n",
        "        except: \n",
        "            return self[random.randint(0, len(self)-1)]\n",
        "\n",
        "def train_gan(data_dir, output_dir, img_size=256, batch_size=8, total_kimg=500, device='cuda', \n",
        "               r1_gamma=0.1, resume_from=None):\n",
        "    \"\"\"\n",
        "    üõ°Ô∏è BULLETPROOF Projected GAN Training\n",
        "    \n",
        "    Features:\n",
        "    - Spectral normalization (in Generator)\n",
        "    - Very conservative learning rates\n",
        "    - Tight gradient clipping\n",
        "    - R1 regularization\n",
        "    - Automatic checkpointing (resume if crash!)\n",
        "    - Collapse detection + early stopping\n",
        "    - LR warmup\n",
        "    - FP32 for Generator (most stable)\n",
        "    \"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # =========================================\n",
        "    # Initialize Models\n",
        "    # =========================================\n",
        "    G = Generator(z_dim=256, img_size=img_size).to(device)\n",
        "    D = ProjectedDiscriminator().to(device)\n",
        "    \n",
        "    # EMA Generator for smoother outputs\n",
        "    G_ema = Generator(z_dim=256, img_size=img_size).to(device)\n",
        "    G_ema.load_state_dict(G.state_dict())\n",
        "    G_ema.eval()\n",
        "    ema_beta = 0.9999  # Slower EMA for stability\n",
        "    \n",
        "    # =========================================\n",
        "    # BULLETPROOF Settings\n",
        "    # =========================================\n",
        "    g_lr = 0.0002   # Very conservative\n",
        "    d_lr = 0.0002\n",
        "    warmup_steps = 500\n",
        "    grad_clip = 0.5  # Tight clipping\n",
        "    r1_interval = 8  # More frequent R1\n",
        "    checkpoint_interval = 5000\n",
        "    max_nan_count = 20  # Stop if too many NaNs\n",
        "    \n",
        "    opt_G = torch.optim.Adam(G.parameters(), lr=g_lr, betas=(0.0, 0.99))\n",
        "    opt_D = torch.optim.Adam(D.parameters(), lr=d_lr, betas=(0.0, 0.99))\n",
        "    \n",
        "    # Only use scaler for D (G runs in FP32 for stability)\n",
        "    scaler_D = GradScaler('cuda')\n",
        "    \n",
        "    dataset = FashionDataset(data_dir, img_size)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, \n",
        "                           num_workers=CONFIG['num_workers'], pin_memory=True, drop_last=True)\n",
        "    fixed_z = torch.randn(16, 256, device=device)\n",
        "    \n",
        "    num_images = len(dataset)\n",
        "    total_samples = total_kimg * 1000\n",
        "    num_epochs = total_samples / num_images\n",
        "    \n",
        "    # =========================================\n",
        "    # Resume from checkpoint if available\n",
        "    # =========================================\n",
        "    start_step = 0\n",
        "    if resume_from and Path(resume_from).exists():\n",
        "        print(f\"üìÇ Loading checkpoint: {resume_from}\")\n",
        "        ckpt = torch.load(resume_from, map_location=device)\n",
        "        G.load_state_dict(ckpt['G'])\n",
        "        G_ema.load_state_dict(ckpt['G_ema'])\n",
        "        D.load_state_dict(ckpt['D'])\n",
        "        opt_G.load_state_dict(ckpt['opt_G'])\n",
        "        opt_D.load_state_dict(ckpt['opt_D'])\n",
        "        start_step = ckpt['step']\n",
        "        print(f\"‚úÖ Resumed from step {start_step}\")\n",
        "    \n",
        "    print(f\"\\n{'='*55}\")\n",
        "    print(f\" üõ°Ô∏è BULLETPROOF GAN Training\")\n",
        "    print(f\"{'='*55}\")\n",
        "    print(f\"üìä Dataset:       {num_images:,} images\")\n",
        "    print(f\"üî¢ Batch size:    {batch_size}\")\n",
        "    print(f\"üîÑ Total kimg:    {total_kimg} ({total_samples:,} samples)\")\n",
        "    print(f\"üìà Epochs:        {num_epochs:.1f}\")\n",
        "    print(f\"‚ö° Learning rate: G={g_lr}, D={d_lr}\")\n",
        "    print(f\"‚úÇÔ∏è Grad clip:     {grad_clip}\")\n",
        "    print(f\"üîí R1 gamma:      {r1_gamma} (every {r1_interval} steps)\")\n",
        "    print(f\"üíæ Checkpoints:   every {checkpoint_interval} steps\")\n",
        "    print(f\"üéØ Generator:     FP32 (most stable)\")\n",
        "    print()\n",
        "    \n",
        "    step = start_step\n",
        "    nan_count = 0\n",
        "    pbar = tqdm(total=total_samples, initial=start_step * batch_size, desc=\"Training\")\n",
        "    \n",
        "    collapsed = False\n",
        "    while step * batch_size < total_kimg * 1000 and not collapsed:\n",
        "        for real in dataloader:\n",
        "            real = real.to(device)\n",
        "            \n",
        "            # =====================================\n",
        "            # Learning Rate Warmup\n",
        "            # =====================================\n",
        "            if step < warmup_steps:\n",
        "                lr_scale = (step + 1) / warmup_steps\n",
        "                for pg in opt_G.param_groups:\n",
        "                    pg['lr'] = g_lr * lr_scale\n",
        "                for pg in opt_D.param_groups:\n",
        "                    pg['lr'] = d_lr * lr_scale\n",
        "            \n",
        "            # =====================================\n",
        "            # Train Discriminator (FP16)\n",
        "            # =====================================\n",
        "            opt_D.zero_grad()\n",
        "            with autocast('cuda'):\n",
        "                z = torch.randn(batch_size, 256, device=device)\n",
        "                with torch.no_grad():\n",
        "                    fake = G(z).detach()\n",
        "                d_loss = hinge_loss_dis(D(real), D(fake))\n",
        "            \n",
        "            if torch.isnan(d_loss) or torch.isinf(d_loss):\n",
        "                nan_count += 1\n",
        "                if nan_count >= max_nan_count:\n",
        "                    tqdm.write(f\"\\n‚ùå COLLAPSE DETECTED! {nan_count} NaNs. Stopping.\")\n",
        "                    collapsed = True\n",
        "                    break\n",
        "                continue\n",
        "            \n",
        "            scaler_D.scale(d_loss).backward()\n",
        "            \n",
        "            # R1 regularization (more frequent for stability)\n",
        "            if step % r1_interval == 0 and r1_gamma > 0:\n",
        "                real_r1 = real.detach().requires_grad_(True)\n",
        "                with autocast('cuda', enabled=False):\n",
        "                    d_real = D(real_r1.float())\n",
        "                    r1_grads = torch.autograd.grad(\n",
        "                        outputs=sum([o.sum() for o in d_real]),\n",
        "                        inputs=real_r1,\n",
        "                        create_graph=True\n",
        "                    )[0]\n",
        "                    r1_penalty = r1_grads.pow(2).sum([1,2,3]).mean() * r1_gamma * 0.5\n",
        "                scaler_D.scale(r1_penalty).backward()\n",
        "            \n",
        "            scaler_D.unscale_(opt_D)\n",
        "            torch.nn.utils.clip_grad_norm_(D.parameters(), grad_clip)\n",
        "            scaler_D.step(opt_D)\n",
        "            scaler_D.update()\n",
        "            \n",
        "            # =====================================\n",
        "            # Train Generator (FP32 for stability!)\n",
        "            # =====================================\n",
        "            opt_G.zero_grad()\n",
        "            \n",
        "            # NO autocast - run G in full FP32\n",
        "            z = torch.randn(batch_size, 256, device=device)\n",
        "            fake = G(z)\n",
        "            \n",
        "            # D can still use FP16 for forward\n",
        "            with autocast('cuda'):\n",
        "                g_loss = hinge_loss_gen(D(fake))\n",
        "            \n",
        "            if torch.isnan(g_loss) or torch.isinf(g_loss):\n",
        "                nan_count += 1\n",
        "                if nan_count >= max_nan_count:\n",
        "                    tqdm.write(f\"\\n‚ùå COLLAPSE DETECTED! {nan_count} NaNs. Stopping.\")\n",
        "                    collapsed = True\n",
        "                    break\n",
        "                opt_G.zero_grad()\n",
        "                continue\n",
        "            else:\n",
        "                nan_count = 0  # Reset on successful step\n",
        "            \n",
        "            g_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(G.parameters(), grad_clip)\n",
        "            opt_G.step()\n",
        "            \n",
        "            # Update EMA Generator\n",
        "            with torch.no_grad():\n",
        "                for p_ema, p in zip(G_ema.parameters(), G.parameters()):\n",
        "                    p_ema.data.mul_(ema_beta).add_(p.data, alpha=1 - ema_beta)\n",
        "            \n",
        "            step += 1\n",
        "            pbar.update(batch_size)\n",
        "            \n",
        "            # =====================================\n",
        "            # Logging\n",
        "            # =====================================\n",
        "            if step % 100 == 0:\n",
        "                curr_epoch = (step * batch_size) / num_images\n",
        "                pbar.set_postfix({\n",
        "                    'Ep': f'{curr_epoch:.1f}',\n",
        "                    'D': f'{d_loss.item():.3f}', \n",
        "                    'G': f'{g_loss.item():.3f}'\n",
        "                })\n",
        "            \n",
        "            # =====================================\n",
        "            # Save samples every 2500 steps\n",
        "            # =====================================\n",
        "            if step % 2500 == 0 and step > 0:\n",
        "                with torch.no_grad():\n",
        "                    samples = G_ema(fixed_z)\n",
        "                    samples = torch.clamp((samples + 1) / 2, 0, 1)\n",
        "                    save_image(samples, output_dir / f'samples_{step:08d}.png', nrow=4)\n",
        "                curr_epoch = (step * batch_size) / num_images\n",
        "                tqdm.write(f\"   üñºÔ∏è Saved samples at step {step} (epoch {curr_epoch:.1f})\")\n",
        "            \n",
        "            # =====================================\n",
        "            # Checkpoint every N steps (CRITICAL!)\n",
        "            # =====================================\n",
        "            if step % checkpoint_interval == 0 and step > 0:\n",
        "                ckpt_path = output_dir / f'checkpoint_{step}.pt'\n",
        "                torch.save({\n",
        "                    'step': step,\n",
        "                    'G': G.state_dict(),\n",
        "                    'G_ema': G_ema.state_dict(),\n",
        "                    'D': D.state_dict(),\n",
        "                    'opt_G': opt_G.state_dict(),\n",
        "                    'opt_D': opt_D.state_dict(),\n",
        "                }, ckpt_path)\n",
        "                tqdm.write(f\"   üíæ Checkpoint saved: {ckpt_path.name}\")\n",
        "            \n",
        "            if step * batch_size >= total_kimg * 1000:\n",
        "                break\n",
        "    \n",
        "    pbar.close()\n",
        "    \n",
        "    # =========================================\n",
        "    # Save Final Model\n",
        "    # =========================================\n",
        "    final_epochs = (step * batch_size) / num_images\n",
        "    \n",
        "    torch.save({\n",
        "        'step': step,\n",
        "        'G': G.state_dict(),\n",
        "        'G_ema': G_ema.state_dict(),\n",
        "        'D': D.state_dict(),\n",
        "        'opt_G': opt_G.state_dict(),\n",
        "        'opt_D': opt_D.state_dict(),\n",
        "    }, output_dir / 'generator_final.pt')\n",
        "    \n",
        "    # Generate final samples\n",
        "    with torch.no_grad():\n",
        "        samples = G_ema(fixed_z)\n",
        "        samples = torch.clamp((samples + 1) / 2, 0, 1)\n",
        "        save_image(samples, output_dir / 'samples_final.png', nrow=4)\n",
        "    \n",
        "    if collapsed:\n",
        "        print(f\"\\n‚ö†Ô∏è Training stopped early due to collapse at step {step}\")\n",
        "        print(f\"   üí° Try reducing learning rate or increasing R1 gamma\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ GAN training complete!\")\n",
        "    \n",
        "    print(f\"   üìä Total steps: {step:,}, Epochs: {final_epochs:.1f}\")\n",
        "    print(f\"   üíæ Saved to {output_dir}\")\n",
        "    print(f\"   üîÑ To resume: set resume_from='{output_dir}/checkpoint_*.pt'\")\n",
        "    \n",
        "    return G_ema\n",
        "\n",
        "print(\"‚úÖ GAN training function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# üõ°Ô∏è BULLETPROOF GAN TRAINING\n",
        "# ============================================\n",
        "# \n",
        "# TRAINING GUIDE for 10k images (with bulletproof settings):\n",
        "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "# ‚îÇ total_kimg   ‚îÇ Epochs  ‚îÇ Est. Time    ‚îÇ Quality        ‚îÇ\n",
        "# ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "# ‚îÇ 500          ‚îÇ 50      ‚îÇ ~2.5hr       ‚îÇ Decent         ‚îÇ\n",
        "# ‚îÇ 1000         ‚îÇ 100     ‚îÇ ~5hr         ‚îÇ Good           ‚îÇ\n",
        "# ‚îÇ 1500         ‚îÇ 150     ‚îÇ ~7.5hr       ‚îÇ Very Good      ‚îÇ\n",
        "# ‚îÇ 2000         ‚îÇ 200     ‚îÇ ~10hr        ‚îÇ Best           ‚îÇ\n",
        "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "# \n",
        "# Note: Bulletproof settings are ~20% slower but WON'T CRASH!\n",
        "# Checkpoints saved every 5000 steps - you can resume if interrupted.\n",
        "\n",
        "GAN_OUTPUT_DIR = OUTPUT_DIR / 'projected_gan'\n",
        "\n",
        "# Use smaller batch for stability\n",
        "gan_batch = min(CONFIG['gan_batch_size'], 8)\n",
        "\n",
        "# To resume from a checkpoint, set this:\n",
        "RESUME_FROM = None  # or: GAN_OUTPUT_DIR / 'checkpoint_5000.pt'\n",
        "\n",
        "generator = train_gan(\n",
        "    data_dir=GAN_DATA_DIR,\n",
        "    output_dir=GAN_OUTPUT_DIR,\n",
        "    img_size=CONFIG['gan_img_size'],\n",
        "    batch_size=gan_batch,\n",
        "    total_kimg=1000,     # 100 epochs - good balance\n",
        "    device=CONFIG['device'],\n",
        "    r1_gamma=0.1,        # Gentle R1 regularization\n",
        "    resume_from=RESUME_FROM,\n",
        ")\n",
        "\n",
        "# Display results\n",
        "if (GAN_OUTPUT_DIR / 'samples_final.png').exists():\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(Image.open(GAN_OUTPUT_DIR / 'samples_final.png'))\n",
        "    plt.title('Generated Fashion Images (Projected GAN)', fontsize=14)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üéØ Part 2: Stable Diffusion + LoRA Training\n",
        "\n",
        "Fine-tune Stable Diffusion v1.5 with LoRA for text-conditioned fashion image generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Stable Diffusion LoRA Imports & Setup\n",
        "# ============================================\n",
        "\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel, StableDiffusionPipeline\n",
        "from diffusers.optimization import get_scheduler\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "class LoRADataset(Dataset):\n",
        "    def __init__(self, data_dir, tokenizer, resolution=512):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.resolution = resolution\n",
        "        self.samples = []\n",
        "        if (self.data_dir / 'metadata.jsonl').exists():\n",
        "            with open(self.data_dir / 'metadata.jsonl') as f:\n",
        "                for line in f:\n",
        "                    item = json.loads(line)\n",
        "                    if (self.data_dir / 'images' / item['file_name']).exists():\n",
        "                        self.samples.append({'path': self.data_dir / 'images' / item['file_name'], 'text': item['text']})\n",
        "        print(f\"Loaded {len(self.samples)} samples\")\n",
        "    \n",
        "    def __len__(self): return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        img = Image.open(sample['path']).convert('RGB').resize((self.resolution, self.resolution), Image.LANCZOS)\n",
        "        if random.random() > 0.5: img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        img = torch.from_numpy(np.array(img).astype(np.float32) / 127.5 - 1.0).permute(2, 0, 1)\n",
        "        tokens = self.tokenizer(sample['text'], max_length=self.tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        return {'pixel_values': img, 'input_ids': tokens.input_ids.squeeze(0)}\n",
        "\n",
        "print(\"‚úÖ LoRA imports ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# LoRA Training Function\n",
        "# ============================================\n",
        "\n",
        "def train_lora(data_dir, output_dir, batch_size=2, grad_accum=4, num_epochs=30, lr=1e-4, lora_rank=64, device='cuda'):\n",
        "    \"\"\"Train Stable Diffusion with LoRA.\"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "    \n",
        "    print(f\"\\n{'='*50}\\n Loading Stable Diffusion v1.5\\n{'='*50}\")\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\").to(device)\n",
        "    vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\").to(device, dtype=torch.float16)\n",
        "    unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\").to(device, dtype=torch.float16)\n",
        "    noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
        "    \n",
        "    vae.requires_grad_(False); text_encoder.requires_grad_(False); unet.requires_grad_(False)\n",
        "    \n",
        "    print(f\"Applying LoRA (rank={lora_rank})...\")\n",
        "    lora_config = LoraConfig(r=lora_rank, lora_alpha=lora_rank, init_lora_weights=\"gaussian\", target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"])\n",
        "    unet = get_peft_model(unet, lora_config)\n",
        "    unet.print_trainable_parameters()\n",
        "    unet.enable_gradient_checkpointing()\n",
        "    try: unet.enable_xformers_memory_efficient_attention(); print(\"‚úì xformers enabled\")\n",
        "    except: pass\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(unet.parameters(), lr=lr)\n",
        "    dataset = LoRADataset(data_dir, tokenizer)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    max_steps = num_epochs * len(dataloader) // grad_accum\n",
        "    lr_scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=max_steps)\n",
        "    \n",
        "    print(f\"\\n{'='*50}\\n Starting LoRA Training\\n{'='*50}\")\n",
        "    print(f\"Epochs: {num_epochs}, Batch: {batch_size}, Grad accum: {grad_accum}, Effective: {batch_size*grad_accum}\\n\")\n",
        "    \n",
        "    global_step = 0\n",
        "    pbar = tqdm(range(max_steps), desc=\"Training\")\n",
        "    unet.train()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            pixels = batch['pixel_values'].to(device, dtype=torch.float16)\n",
        "            ids = batch['input_ids'].to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                latents = vae.encode(pixels).latent_dist.sample() * vae.config.scaling_factor\n",
        "                encoder_hidden = text_encoder(ids)[0]\n",
        "            \n",
        "            noise = torch.randn_like(latents)\n",
        "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
        "            noisy = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "            \n",
        "            with torch.autocast('cuda', dtype=torch.float16):\n",
        "                pred = unet(noisy, timesteps, encoder_hidden).sample\n",
        "            \n",
        "            loss = F.mse_loss(pred.float(), noise.float()) / grad_accum\n",
        "            loss.backward()\n",
        "            \n",
        "            if (step + 1) % grad_accum == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
        "                optimizer.step(); lr_scheduler.step(); optimizer.zero_grad()\n",
        "                global_step += 1; pbar.update(1)\n",
        "                if global_step % 10 == 0: pbar.set_postfix({'loss': f'{loss.item()*grad_accum:.4f}', 'lr': f'{lr_scheduler.get_last_lr()[0]:.2e}'})\n",
        "                if global_step >= max_steps: break\n",
        "        if global_step >= max_steps: break\n",
        "    \n",
        "    pbar.close()\n",
        "    unet.save_pretrained(output_dir / 'checkpoint-final')\n",
        "    print(f\"\\n‚úÖ LoRA training complete! Saved to {output_dir / 'checkpoint-final'}\")\n",
        "    return unet, vae, text_encoder, tokenizer\n",
        "\n",
        "print(\"‚úÖ LoRA training function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TRAIN LoRA\n",
        "# ============================================\n",
        "\n",
        "LORA_OUTPUT_DIR = OUTPUT_DIR / 'lora'\n",
        "\n",
        "unet, vae, text_encoder, tokenizer = train_lora(\n",
        "    data_dir=LORA_DATA_DIR,\n",
        "    output_dir=LORA_OUTPUT_DIR,\n",
        "    batch_size=CONFIG['lora_batch_size'],\n",
        "    grad_accum=CONFIG['lora_grad_accum'],\n",
        "    num_epochs=30,  # Adjust based on time\n",
        "    lr=1e-4,\n",
        "    lora_rank=64,\n",
        "    device=CONFIG['device'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Generate Images with LoRA\n",
        "# ============================================\n",
        "\n",
        "def generate_images(unet, vae, text_encoder, tokenizer, prompts, output_dir, device='cuda'):\n",
        "    \"\"\"Generate images using trained LoRA.\"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    \n",
        "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\", unet=unet, text_encoder=text_encoder, vae=vae, torch_dtype=torch.float16\n",
        "    ).to(device)\n",
        "    pipeline.safety_checker = None\n",
        "    \n",
        "    print(\"\\nGenerating images...\")\n",
        "    images = []\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        print(f\"  [{i+1}/{len(prompts)}] {prompt[:50]}...\")\n",
        "        img = pipeline(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
        "        img.save(output_dir / f'generated_{i:03d}.png')\n",
        "        images.append(img)\n",
        "    \n",
        "    # Display grid\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    for ax, img, prompt in zip(axes.flatten(), images, prompts):\n",
        "        ax.imshow(img); ax.set_title(prompt[:40]+'...', fontsize=9); ax.axis('off')\n",
        "    plt.suptitle('Generated Fashion Images (SD + LoRA)', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_dir / 'generated_grid.png', dpi=150)\n",
        "    plt.show()\n",
        "    \n",
        "    del pipeline; torch.cuda.empty_cache()\n",
        "    return images\n",
        "\n",
        "# Test prompts\n",
        "prompts = [\n",
        "    \"a high quality fashion photograph of an elegant red dress, studio lighting\",\n",
        "    \"professional product photo of a black leather jacket, minimalist background\",\n",
        "    \"fashion photography of blue denim jeans, white background, sharp focus\",\n",
        "    \"luxury fashion photograph of a silk blouse, soft lighting, professional\",\n",
        "    \"high-end fashion product shot of designer sneakers, clean background\",\n",
        "    \"professional fashion photograph of a wool coat, autumn fashion, detailed\",\n",
        "]\n",
        "\n",
        "generated = generate_images(unet, vae, text_encoder, tokenizer, prompts, LORA_OUTPUT_DIR, CONFIG['device'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üì¶ Save & Download Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Summary & Download\n",
        "# ============================================\n",
        "import zipfile\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" üéâ Training Complete!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüìÅ Output Files:\")\n",
        "print(f\"\\n  Projected GAN: {GAN_OUTPUT_DIR}\")\n",
        "for f in sorted(GAN_OUTPUT_DIR.glob('*'))[:5]:\n",
        "    print(f\"    - {f.name}\")\n",
        "\n",
        "print(f\"\\n  Stable Diffusion LoRA: {LORA_OUTPUT_DIR}\")\n",
        "for f in sorted(LORA_OUTPUT_DIR.glob('*'))[:5]:\n",
        "    print(f\"    - {f.name}\")\n",
        "\n",
        "# Create zip files for download\n",
        "def zip_dir(path, zip_name):\n",
        "    zip_path = Path('/kaggle/working') / zip_name\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:\n",
        "        for f in Path(path).rglob('*'):\n",
        "            if f.is_file(): z.write(f, f.relative_to(path))\n",
        "    print(f\"Created {zip_path} ({zip_path.stat().st_size / 1e6:.1f} MB)\")\n",
        "\n",
        "zip_dir(GAN_OUTPUT_DIR, 'gan_results.zip')\n",
        "zip_dir(LORA_OUTPUT_DIR, 'lora_results.zip')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" Download from 'Output' tab on the right ‚Üí\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
